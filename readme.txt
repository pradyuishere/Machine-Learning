Python version 3.6.5 
Also, a1 in the name of the python file indicates the question belongs to hw0 and similarly a2 indicates a question from hw1.

Problem 1: This plots a straight line regression. We can clearly observe our model not entirely capturing the relation between the input and output labels.

Problem 2: Problem 1 is a simple sub problem of this. Here, we take a higher order polynomial features as input labels. As we increase the polynomial order, squared sum error constantly decreases but we can also clearly see that the same model starts performing on the new test data worse as the polynomial order increases due to overfitting. If the polynomial order is too low, the model cannot successfully capture the entire functional relationship between the inputs and the outputs.

Problem 3: This problem is an extension of problem 2. Here we introduce the regularisation parameter Lambda. Putting the Lambda, the regularisation parameter to zero, we observe no difference between the model obtained from problem 2. But increasing Lambda when we have a high degree polynomial, we see the performance on the test data increase gradually but only until a certain point. Beyond a certain value, the system becomes too regularised and again fails to capture the functional relatonship in the input Labels. Also, when we use regularisation, it tries to tune down the effect of higher order polynomials. Due to this, if the input degree is too small, regularisation results in underfitting.

Problem 4: Here, the variance between the predictions and the output labels is observed. Also for different values of varience of noise added to the training outputs, the performance also varies. Adding noise with high varience makes the model less likely to adapt with the actual functional relationship between the input and output Labels. Also, increasing the noise and the number of parameters in the data, we can observe increasing overfitting.

Problem 5: Here, the standard deviation sigma and the standard deviation of the weights for the maximum likelihood are observed. As the values of sigma and alpha are changed, we have a noticable change in the model behaviour. As the value of sigma is increased or the value of alpha is decreased, a higher order model starts to overfit the data. Also, as we decrease the sigma or increase the alpha by a substantial amount, the model begins underfitting the data. But for an optimum value of Alpha nd sigma, say the polynomial order is 7, sigma=0.01, alpha=0.009 and many other sets of values, the models performs great with the test data as well.
